{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03c77969-5f06-4547-aca6-81fef52ef864",
   "metadata": {},
   "source": [
    "# Running RAG experiments using S3 vectors and MLflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e144d4-e9e9-424d-bf09-087b9bfd4e91",
   "metadata": {},
   "source": [
    "## 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e48751ec-c06b-4336-bfa4-5ed08cc3b965",
   "metadata": {},
   "source": [
    "The following notebook illustrates the process of experimenting with different chunking, retrieval, and generation configurations when building a RAG (retrieval augmented generation) solution. In this case, we are going to use S3 vectors to store the embedding vectors, and MLflow to run evaluations and to store parameters from the experiments.\n",
    "\n",
    "![Architecture](s3-vectors-buckets-arch.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfa9f91f-5e9c-48eb-b636-67ecd87f347c",
   "metadata": {},
   "source": [
    "## 2. Global parameters for the experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35500cd8",
   "metadata": {},
   "source": [
    "Let us set up some global parameters that we are going to use in this round of experiments. These parameters can be, for example, the model ID and the version of the SageMaker JumpStart models we are going to use, and the chunking parameters for processing the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e7231aa-daee-48d6-917b-a38aa32751dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_MODEL_ID = \"huggingface-textembedding-gte-qwen2-7b-instruct\"\n",
    "EMBEDDING_MODEL_VERSION = \"1.0.10\"\n",
    "NUM_DIMENSIONS = 3584 # Number of dimensions in the embedding model\n",
    "\n",
    "TEXT_GENERATION_MODEL_ID = \"deepseek-llm-r1-distill-qwen-7b\"\n",
    "TEXT_GENERATION_MODEL_VERSION = \"2.0.5\"\n",
    "\n",
    "CHUNK_SIZE = 1000\n",
    "CHUNK_OVERLAP = 250"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a714988a-6ed5-4223-9921-4a97b3362488",
   "metadata": {},
   "source": [
    "## 3. Install required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aacfbf1a-b75f-4dd3-be5c-43438e272998",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install boto3 botocore mlflow sagemaker-mlflow langchain-text-splitters langgraph pypdf --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad33fd61-dc3e-4203-a06b-32e7fc2a4c8c",
   "metadata": {},
   "source": [
    "## 4. Deploy embedding model and text generation model\n",
    "\n",
    "Retrieval augmented generation (RAG) requires two types of models:\n",
    "\n",
    "- Embedding model: To map text chunks into an embeddings space to perform similarity search.\n",
    "- Text generation model: To generate responses based on the text chunks retrieved.\n",
    "\n",
    "The following cell deploys the models from SageMaker Jumpstart and stores their names so we can use them later. To keep the code concise, several helper functions have been defined in the `utils.py` file and are imported in this notebook. We are using, for example, the `deploy_jumpstart_model` function to deploy models from SageMaker JumpStart.\n",
    "\n",
    "This cell can take ~20 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc18ea6-4080-4984-bb7b-e7325f659d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.jumpstart.model import JumpStartModel\n",
    "import sagemaker\n",
    "\n",
    "def deploy_jumpstart_model(model_id: str,\n",
    "                 instance_type: str,\n",
    "                 endpoint_name_base: str,\n",
    "                 model_version: str = \"*\") -> str:\n",
    "    \"\"\"\n",
    "    Deploys a SageMaker JumpStart model.\n",
    "\n",
    "    Args:\n",
    "        model_id (str): The JumpStart model ID.\n",
    "        instance_type (str): The SageMaker instance type.\n",
    "        endpoint_name_base (str): Prefix for the endpoint name.\n",
    "        model_version (str): The version of the model to deploy.\n",
    "\n",
    "    Returns:\n",
    "        str: Name of the deployed real-time endpoint\n",
    "    \"\"\"\n",
    "    endpoint_name = sagemaker.utils.unique_name_from_base(endpoint_name_base)\n",
    "    model = JumpStartModel(\n",
    "        model_id=model_id,\n",
    "        model_version=model_version,\n",
    "        instance_type=instance_type\n",
    "    )\n",
    "    model.deploy(\n",
    "        accept_eula=True,\n",
    "        initial_instance_count=1,\n",
    "        instance_type=instance_type,\n",
    "        endpoint_name=endpoint_name\n",
    "    )\n",
    "\n",
    "    return endpoint_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195c4ebe-d21e-4b04-b7ce-0d40152c798e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "EMBEDDING_ENDPOINT_NAME = deploy_jumpstart_model(\n",
    "    model_id=EMBEDDING_MODEL_ID,\n",
    "    instance_type=\"ml.g5.xlarge\",\n",
    "    endpoint_name_base=\"s3-vectors-demo-embedding-model\",\n",
    "    model_version=EMBEDDING_MODEL_VERSION\n",
    "    )\n",
    "print(f\"EMBEDDING_ENDPOINT_NAME:: {EMBEDDING_ENDPOINT_NAME}\")\n",
    "\n",
    "TEXT_GENERATION_ENDPOINT_NAME = deploy_jumpstart_model(\n",
    "    model_id=TEXT_GENERATION_MODEL_ID,\n",
    "    instance_type=\"ml.g6.12xlarge\",\n",
    "    endpoint_name_base=\"s3-vectors-demo-generation-model\",\n",
    "    model_version=TEXT_GENERATION_MODEL_VERSION\n",
    ")\n",
    "\n",
    "print(f\"TEXT_GENERATION_ENDPOINT_NAME:: {TEXT_GENERATION_ENDPOINT_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4234c4f-7e7d-47d8-8e80-d858f6a3a9dc",
   "metadata": {},
   "source": [
    "## 5. Create vectors bucket and vector indexes\n",
    "\n",
    "We are going to create an S3 vectors bucket to store the vectors, and inside it, we are going to have two vector indexes, one for fixed-size chunking and one for recursive chunking. A vector index is a logical group for vector data, and it is used for all write and read requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7323c77-ffce-4594-a706-b16548186b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker.utils import unique_name_from_base\n",
    "\n",
    "VECTORS_BUCKET_NAME = unique_name_from_base(\"s3-vectors-bucket\")\n",
    "FIXED_CHUNKING_INDEX_NAME = \"fixed-chunking\"\n",
    "RECURSIVE_CHUNKING_INDEX_NAME = \"recursive-chunking\"\n",
    "\n",
    "print(f\"VECTORS_BUCKET_NAME:: {VECTORS_BUCKET_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0194711-62f6-4aa3-83d0-fa7809b84d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3vectors_client = boto3.client(\"s3vectors\")\n",
    "\n",
    "s3vectors_client.create_vector_bucket(\n",
    "    vectorBucketName=VECTORS_BUCKET_NAME\n",
    "    )\n",
    "\n",
    "for index_name in [FIXED_CHUNKING_INDEX_NAME, RECURSIVE_CHUNKING_INDEX_NAME]:\n",
    "    s3vectors_client.create_index(\n",
    "        vectorBucketName=VECTORS_BUCKET_NAME,\n",
    "        indexName=index_name,\n",
    "        dimension=NUM_DIMENSIONS,\n",
    "        distanceMetric=\"euclidean\",\n",
    "        dataType=\"float32\",\n",
    "        metadataConfiguration={\"nonFilterableMetadataKeys\": [\"chunk\"]}\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48bd3a89-dc2d-4eaa-aa6e-95bcc74d1fc3",
   "metadata": {},
   "source": [
    "## 6. Sample datasets\n",
    "\n",
    "The dataset is ingested based on a JSON file which has the source URL of each document and associated meta-data. The files are downloaded and stored locally programatically using the JSON file as reference. You need to update the JSON file with the source URL and metadata of the documents you want to use as data source.\n",
    "\n",
    "In the sample dataset, we are using publicly available financial data of amazon.com. There are two versions of this same json file - the `source_files_short_version.json` has reference for 3 documents whereas the other one has 9 documents. The default is the long version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae93701b-00f4-4d96-b161-343abc3fea54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import download_pdfs, process_pdf\n",
    "import json\n",
    "\n",
    "short_version = True\n",
    "\n",
    "file_path = \"source_files_short_version.json\" if short_version else \"source_files_long_version.json\"\n",
    "with open(file_path, 'r') as file:\n",
    "    json_data = json.load(file)\n",
    "\n",
    "urls, filenames, metadata = (json_data.get(k, []) for k in ('url', 'document', 'metadata'))\n",
    "\n",
    "download_pdfs(urls, filenames)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d25c07-c1e3-4ca9-9567-89e327120602",
   "metadata": {},
   "source": [
    "## 7. Calculate embedding vectors\n",
    "\n",
    "We are now going to define a function that calculates the embedding vector for text chunks. This function is going to use an embedding model to calculate the vectors. Both the vectors and the text chunks are stored in an S3 vector bucket, and metadata is added to each vector using the `domain` key to indicate its origin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbedd03f-97ad-495e-bf51-7dc16e75d0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "import uuid\n",
    "from botocore.exceptions import ClientError\n",
    "from typing import List, Dict\n",
    "from sagemaker.predictor import Predictor\n",
    "\n",
    "def calculate_embedding_vectors(\n",
    "    chunks: List[Dict[str, str]],\n",
    "    embeddings_model_endpoint: Predictor,\n",
    "    vector_bucket_name: str,\n",
    "    index_name: str,\n",
    "    domain_name: str,\n",
    "    year: int\n",
    "):\n",
    "    \"\"\"\n",
    "    Calculate embeddings vector and store them in the vector bucket.\n",
    "\n",
    "    Args:\n",
    "        chunks (List[Dict[str, str]]): A list of dictionaries, where each dictionary contains a chunk of text.\n",
    "        embeddings_model_endpoint (Predictor): A SageMaker Predictor instance used to calculate embeddings.\n",
    "        vector_bucket_name (str): The name of the vector bucket to store vector data.\n",
    "        index_name (str): The vector index within the vector bucket.\n",
    "        domain_name (str): The domain metadata to be added to the vectors.\n",
    "        year (int): The year of the document\n",
    "    \"\"\"\n",
    "    s3vectors_client = boto3.client(\"s3vectors\")\n",
    "\n",
    "    for chunk in chunks:\n",
    "        unique_id = str(uuid.uuid4())\n",
    "        key = f\"{unique_id}\"\n",
    "\n",
    "        embedding_response = embeddings_model_endpoint.predict({'inputs': chunk[\"chunk\"]})\n",
    "\n",
    "        s3vectors_client.put_vectors(\n",
    "            vectorBucketName=vector_bucket_name,\n",
    "            indexName=index_name,\n",
    "            vectors=[\n",
    "                {\n",
    "                    \"key\": key,\n",
    "                    \"data\": {\n",
    "                        \"float32\": embedding_response[0]\n",
    "                    },\n",
    "                    \"metadata\": {\n",
    "                        \"domain\": domain_name,\n",
    "                        \"year\": year,\n",
    "                        \"chunk\": chunk[\"chunk\"]\n",
    "                    }\n",
    "                }\n",
    "            ]\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0830f942-eeba-4e33-b568-91223710e69b",
   "metadata": {},
   "source": [
    "We now define a SageMaker Predictor based on the endpoint defined above for the embedding model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e03d52-6006-4cf5-a24c-677ecbe96204",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.serializers import JSONSerializer, IdentitySerializer\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "from sagemaker.predictor import Predictor\n",
    "\n",
    "embedding_model_predictor = Predictor(\n",
    "    endpoint_name=EMBEDDING_ENDPOINT_NAME,\n",
    "    serializer=JSONSerializer(),\n",
    "    deserializer=JSONDeserializer()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e5210d2-3a6a-4dca-8ef5-6748e8f79e1f",
   "metadata": {},
   "source": [
    "## 8. Storing embedding vectors for fixed-size and recursive chunking\n",
    "\n",
    "Now we are going to iterate through the files, process the content from the PDFs, and load the vectors as `records` in each vector index. We will keep a separate vector indexes for recursive chunking and fixed-size chunking, so we can compare their performance in RAG solutions later.\n",
    "\n",
    "The next cells can take approximately 10 minutes each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a60d9862-2d82-4249-8079-b9740210cd6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "num_chunks = 0\n",
    "\n",
    "# Iterate through the files to extract chunks, and store them in the S3 vectors bucket\n",
    "for filename, meta in zip(filenames, metadata):\n",
    "    extracted_chunks = process_pdf(\n",
    "        filename, chunking=\"recursive\",\n",
    "        chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP\n",
    "    )\n",
    "    num_chunks += len(extracted_chunks)\n",
    "    print(f\"filename:: {filename}\")\n",
    "\n",
    "    calculate_embedding_vectors(\n",
    "        chunks=extracted_chunks,\n",
    "        embeddings_model_endpoint=embedding_model_predictor,\n",
    "        vector_bucket_name=VECTORS_BUCKET_NAME,\n",
    "        index_name=RECURSIVE_CHUNKING_INDEX_NAME,\n",
    "        domain_name=meta[\"domain\"],\n",
    "        year=meta[\"year\"]\n",
    "    )\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "CHUNKING_TIME_RECURSIVE = end_time-start_time\n",
    "print(f\"Elapsed time: {CHUNKING_TIME_RECURSIVE:.0f} seconds\")\n",
    "NUM_CHUNKS_RECURSIVE = num_chunks\n",
    "print(f\"Number of text chunks: {NUM_CHUNKS_RECURSIVE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ca79cf-4496-4dd6-a977-e81d098eb83f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "num_chunks = 0\n",
    "\n",
    "# Iterate through the files to extract chunks, and store them in the S3 vectors bucket\n",
    "for filename, meta in zip(filenames, metadata):\n",
    "    extracted_chunks = process_pdf(\n",
    "        filename, chunking=\"fixed\",\n",
    "        chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP\n",
    "    )\n",
    "    num_chunks += len(extracted_chunks)\n",
    "    print(f\"filename:: {filename}\")\n",
    "\n",
    "    calculate_embedding_vectors(\n",
    "        chunks=extracted_chunks,\n",
    "        embeddings_model_endpoint=embedding_model_predictor,\n",
    "        vector_bucket_name=VECTORS_BUCKET_NAME,\n",
    "        index_name=FIXED_CHUNKING_INDEX_NAME,\n",
    "        domain_name=meta[\"domain\"],\n",
    "        year=meta[\"year\"]\n",
    "    )\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "CHUNKING_TIME_FIXED = end_time-start_time\n",
    "print(f\"Elapsed time: {CHUNKING_TIME_FIXED:.0f} seconds\")\n",
    "NUM_CHUNKS_FIXED = num_chunks\n",
    "print(f\"Number of text chunks: {NUM_CHUNKS_FIXED}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db2ff31c-6385-421a-8cf1-0e210032d9e8",
   "metadata": {},
   "source": [
    "## 9. Run simple semantic search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7498c931-46c3-4587-ad83-af8e68794206",
   "metadata": {},
   "source": [
    "We are first going to test only the **retrieval** part by running semantic search within a vector index. When running a query with a string, the steps will be:\n",
    "\n",
    "- Use the embedding model to compute the query vector\n",
    "\n",
    "- Query the corresponding vector index using the query vector. The `metadata_filter` argument is optional and it may be used if we want to restrict the semantic search to a subset of document that come from a particular `domain`\n",
    "\n",
    "- Extract the text chunks from the metadata of the returned records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "858a025d-42a6-4b83-8952-ddfdbf9a3775",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "from typing import List, Dict, Any, Optional\n",
    "\n",
    "def query_vectors_with_text(\n",
    "    text: str,\n",
    "    embeddings_model_endpoint: Predictor,\n",
    "    vector_bucket_name: str,\n",
    "    index_name: str,\n",
    "    top_K: int = 5,\n",
    "    metadata_filter: Optional = None,\n",
    ") -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Queries a vector index using a text input and returns the nearest neighbors.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input text to query.\n",
    "        embeddings_model_endpoint (Predictor): The endpoint for generating embeddings.\n",
    "        vector_bucket_name (str): The name of the vector bucket.\n",
    "        index_name (str): The name of the vector index to query.\n",
    "        top_K (int): Number of neighbors to fetch.\n",
    "        metadata_filter: The filters to apply.\n",
    "\n",
    "    Returns:\n",
    "        List[Dict[str, Any]]: A list of neighbors with their metadata and corresponding text chunks.\n",
    "    \"\"\"\n",
    "    query_vector = embeddings_model_endpoint.predict({'inputs': text})[0]\n",
    "\n",
    "    s3vectors_client = boto3.client(\"s3vectors\")\n",
    "\n",
    "    query_args = {\n",
    "        \"vectorBucketName\": vector_bucket_name,\n",
    "        \"indexName\": index_name,\n",
    "        \"queryVector\": {\"float32\": query_vector},\n",
    "        \"topK\": top_K,\n",
    "        \"returnDistance\": True,\n",
    "        \"returnMetadata\": True\n",
    "    }\n",
    "\n",
    "    if metadata_filter is not None:\n",
    "        query_args[\"filter\"] = metadata_filter\n",
    "\n",
    "    vectors = s3vectors_client.query_vectors(**query_args).get(\"vectors\", [])\n",
    "    neighbors = [v.get(\"data\", {}).get(\"float32\") for v in vectors]\n",
    "    chunks = [v.get(\"metadata\", {}).get(\"chunk\") for v in vectors]\n",
    "\n",
    "    return neighbors, chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "816bf090-e528-4511-9c02-c13f2d416e4c",
   "metadata": {},
   "source": [
    "Now we can run a sample semantic query. We are going to inspect only Amazon financial documents using the `metadata_filter` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235f4210-9de9-44d6-aa58-5a893be6bba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import random\n",
    "\n",
    "_, chunks = query_vectors_with_text(\n",
    "    text=\"How old is Jeff Bezos?\",\n",
    "    embeddings_model_endpoint=embedding_model_predictor,\n",
    "    vector_bucket_name=VECTORS_BUCKET_NAME,\n",
    "    index_name=RECURSIVE_CHUNKING_INDEX_NAME,\n",
    "    top_K = 1,\n",
    "    metadata_filter={\n",
    "        \"$and\": [\n",
    "            {\"domain\": {\"$eq\": \"Amazon Financial Report\"}},\n",
    "            {\"year\": {\"$eq\": 2025}}\n",
    "            ]\n",
    "        }\n",
    "    )\n",
    "\n",
    "print(f\"extracted_chunks count:: {len(chunks)}\")\n",
    "retrieved_chunk = random.choice(chunks)\n",
    "print(f\"Sample chunk:\\n {retrieved_chunk}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ba4827-0c5c-4f06-81ce-14f80426f1a3",
   "metadata": {},
   "source": [
    "The sampled chunk should show some semantic similarity to the query string, probably by mentioning Amazon's board of directors in some form, if not by directly mentioning its members. Now let's try a different query without the metadata filter. This query will inspect more candidate text chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ff2bce-811a-4463-bfb3-62ccc7214118",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import random\n",
    "\n",
    "_, chunks = query_vectors_with_text(\n",
    "    text=\"Who are the executive officers mentioned in Amazon financial report?\",\n",
    "    embeddings_model_endpoint=embedding_model_predictor,\n",
    "    vector_bucket_name=VECTORS_BUCKET_NAME,\n",
    "    index_name=RECURSIVE_CHUNKING_INDEX_NAME\n",
    "    )\n",
    "\n",
    "print(f\"extracted_chunks count:: {len(chunks)}\")\n",
    "retrieved_chunk = random.choice(chunks)\n",
    "print(f\"Sample chunk:\\n {retrieved_chunk}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be6884c-a1fe-4af0-a18f-f3ca9a1dfc2d",
   "metadata": {},
   "source": [
    "Let's now inspect a different domain by focusing on the annual report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c7ecdd-7099-4410-9079-89ec853adcec",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import random\n",
    "\n",
    "_, chunks = query_vectors_with_text(\n",
    "    text=\"What are the business and industry risk called out in Amazon's 2024 annual report?\",\n",
    "    embeddings_model_endpoint=embedding_model_predictor,\n",
    "    vector_bucket_name=VECTORS_BUCKET_NAME,\n",
    "    index_name=RECURSIVE_CHUNKING_INDEX_NAME,\n",
    "    metadata_filter={\"domain\": {\"$eq\": \"Amazon Annual Report\"}}\n",
    "    )\n",
    "\n",
    "print(f\"extracted_chunks count:: {len(chunks)}\")\n",
    "retrieved_chunk = random.choice(chunks)\n",
    "print(f\"Sample chunk:\\n {retrieved_chunk}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06283406-0065-4638-b219-eaf9d9b9c9ab",
   "metadata": {},
   "source": [
    "## 10. Retrieval augmented generation\n",
    "\n",
    "Now we are going to put the semantic search in conjunction with a generation step to have a complete retrieval augmented generation call. This requires two steps, so we are going to define a `StateGraph` from `LangGraph` to define the flow between the `retrieve` and `generate` steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "358dddfb-edba-4822-b87a-79bbac29b3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.serializers import JSONSerializer, IdentitySerializer\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "from sagemaker.predictor import Predictor\n",
    "\n",
    "# Load the text generation model as a SageMaker Predictor\n",
    "text_generation_predictor = Predictor(\n",
    "    endpoint_name=TEXT_GENERATION_ENDPOINT_NAME,\n",
    "    serializer=JSONSerializer(),\n",
    "    deserializer=JSONDeserializer()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33680508-d092-412e-99cb-7c4a7ac418bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.messages import BaseMessage\n",
    "from langchain.schema import HumanMessage, AIMessage, SystemMessage\n",
    "from langgraph.graph import START, StateGraph\n",
    "from typing import List, Dict, Union, Any\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "from utils import langchain_to_openai_messages\n",
    "\n",
    "# Load RAG prompt from LangChain Hub\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "# Define application state structure\n",
    "class State(TypedDict):\n",
    "    question: str\n",
    "    context: List[Document]\n",
    "    answer: str\n",
    "\n",
    "def retrieve(state: State, chunking_strategy: str = \"recursive\") -> Dict[str, List[Document]]:\n",
    "    \"\"\"\n",
    "    Retrieves relevant documents for a given question using vector search.\n",
    "    \n",
    "    Args:\n",
    "        state (State): The current state containing the question.\n",
    "        chunking_strategy (str): Either \"recursive\" or \"fixed\" to control the vector index used.\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, List[Document]]: A dictionary with retrieved context documents.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        index_name = (\n",
    "            RECURSIVE_CHUNKING_INDEX_NAME if chunking_strategy == \"recursive\"\n",
    "            else FIXED_CHUNKING_INDEX_NAME\n",
    "        )\n",
    "\n",
    "        _, chunks = query_vectors_with_text(\n",
    "            text=state[\"question\"],\n",
    "            embeddings_model_endpoint=embedding_model_predictor,\n",
    "            vector_bucket_name=VECTORS_BUCKET_NAME,\n",
    "            index_name=index_name,\n",
    "        )\n",
    "        document_chunks = [Document(page_content=chunk) for chunk in chunks]\n",
    "        return {\"context\": document_chunks}\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Document retrieval failed: {e}\")\n",
    "\n",
    "\n",
    "def generate(state: State) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Generates an answer using retrieved context and user question.\n",
    "\n",
    "    Args:\n",
    "        state (State): The current state containing question and context documents.\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, str]: A dictionary with the generated answer.\n",
    "\n",
    "    Raises:\n",
    "        RuntimeError: If text generation fails or returns an unexpected response.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        docs_content = \"\\n\\n\".join(doc.page_content for doc in state[\"context\"])\n",
    "\n",
    "        # Generate LangChain messages and convert to OpenAI format\n",
    "        lc_messages = prompt.invoke({\n",
    "            \"question\": state[\"question\"],\n",
    "            \"context\": docs_content\n",
    "        }).to_messages()\n",
    "        openai_messages = langchain_to_openai_messages(lc_messages)\n",
    "\n",
    "        request = {\n",
    "            \"messages\": openai_messages,\n",
    "            \"temperature\": 0.2,\n",
    "            \"max_new_tokens\": 512,\n",
    "        }\n",
    "\n",
    "        response = text_generation_predictor.predict(request)\n",
    "\n",
    "        # Validate response structure\n",
    "        if (\"choices\" not in response \n",
    "                or not response[\"choices\"] \n",
    "                or \"message\" not in response[\"choices\"][0]\n",
    "                or \"content\" not in response[\"choices\"][0][\"message\"]):\n",
    "            raise ValueError(\"Unexpected response format from text generation predictor.\")\n",
    "\n",
    "        return {\"answer\": response[\"choices\"][0][\"message\"][\"content\"]}\n",
    "\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Text generation failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4436e00b",
   "metadata": {},
   "source": [
    "The following cells assembles the functions defined above into a LangGraph `StateGraph` with two nodes: `retrieve` and `generate`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e94617-6cf5-4a50-a77d-77f2ce07d171",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from langgraph.graph.state import StateGraph\n",
    "\n",
    "def build_graph(chunking_strategy: str) -> StateGraph:\n",
    "    \"\"\"\n",
    "    Build LangGraph graph for RAG\n",
    "\n",
    "    Args:\n",
    "        chunking_strategy (str): Define the preferred chunking strategy.\n",
    "\n",
    "    Returns:\n",
    "        graph (StateGraph): LangGraph StateGraph.\n",
    "    \"\"\"\n",
    "    # Build graph explicitly adding the node with a name\n",
    "    graph_builder = StateGraph(State)\n",
    "    graph_builder.add_node(\"retrieve\", partial(retrieve, chunking_strategy=chunking_strategy))\n",
    "    graph_builder.add_node(\"generate\", generate)\n",
    "    # Define edges\n",
    "    graph_builder.set_entry_point(\"retrieve\")\n",
    "    graph_builder.add_edge(\"retrieve\", \"generate\")\n",
    "\n",
    "    return graph_builder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b822d4-9ecd-46c7-b697-8e57bd324833",
   "metadata": {},
   "source": [
    "We are going to build and compile two different `StateGraphs`, one for querying the vector index with vectors obtained through `recursive` chunking, and the other one to focus on those obtained with `fixed` sized chunking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e2cd76-de2c-46e8-b1cd-1827cfc5fd8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the graphs\n",
    "recursive_graph_builder = build_graph(chunking_strategy=\"recursive\")\n",
    "recursive_graph = recursive_graph_builder.compile()\n",
    "\n",
    "fixed_graph_builder = build_graph(chunking_strategy=\"fixed\")\n",
    "fixed_graph = fixed_graph_builder.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b81f108-5668-4396-b82d-8ec564830efb",
   "metadata": {},
   "source": [
    "Let's test the graphs by invoking them with a simple question. This time, rather than simply retrieving text chunks that are semantically similar, the graph will als use these chunking when composing a response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31cfd37-766d-4358-b19b-52df9f535824",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "response = recursive_graph.invoke(\n",
    "    {\n",
    "        \"question\": \"What are the names of the people in Amazon's board of directors?\"\n",
    "    }\n",
    "    )\n",
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43169b4-307f-4987-adf7-7df6105a50fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "response = fixed_graph.invoke(\n",
    "    {\n",
    "        \"question\": \"What are the names of the people in Amazon's board of directors?\"\n",
    "    }\n",
    "    )\n",
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111a9a2b-d90a-493b-8fb1-80cf367a7d51",
   "metadata": {},
   "source": [
    "In this case, both chunking strategies should succeed in generating accurate responses to the question."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d05fd4-8672-4fb7-b247-c0a50d6ebdb3",
   "metadata": {},
   "source": [
    "## 11. Evaluating the RAG solutions using MLflow\n",
    "\n",
    "Now we have a `LangGraph` graph to run RAG, it would be good to know how good it is. This is critical in the experimentation phase because we could use this information to adjust the chunking method, chunk size, and the models for better performance.\n",
    "\n",
    "We are going to use a ground truth data set in the form of questions and answers in a JSONL file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c93256-c2d6-4ce1-93f0-b97dd345129b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "eval_df = pd.read_json(\"amazon_10k_eval_dataset.jsonl\", lines=True)\n",
    "eval_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d63bf02-2a8a-4b08-9675-bc7b6e979623",
   "metadata": {},
   "source": [
    "For the evaluation, we are going to use [MLflow LLM metrics](https://mlflow.org/docs/latest/llms/llm-evaluate/), which require the definition of `model` functions. We are going to create a different function to invoke each `StateGraph` graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "205d1dae-cc00-4b54-a631-0cf714d7c1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recursive_model(input_df: pd.DataFrame) -> List[str]:\n",
    "    \"\"\"\n",
    "    Takes a pandas DataFrame with a 'question' column, passes each question \n",
    "    to a LangGraph model for inference, and returns a list of string answers.\n",
    "\n",
    "    This function is intended for use with MLflow evaluation workflows.\n",
    "    \"\"\"\n",
    "    answer = []\n",
    "    for index, row in input_df.iterrows():\n",
    "        response = recursive_graph.invoke({\n",
    "            \"question\": row[\"question\"]\n",
    "        })\n",
    "        answer.append(response[\"answer\"])\n",
    "\n",
    "    return answer\n",
    "\n",
    "def fixed_model(input_df: pd.DataFrame) -> List[str]:\n",
    "    \"\"\"\n",
    "    Takes a pandas DataFrame with a 'question' column, passes each question \n",
    "    to a LangGraph model for inference, and returns a list of string answers.\n",
    "\n",
    "    This function is intended for use with MLflow evaluation workflows.\n",
    "    \"\"\"\n",
    "    answer = []\n",
    "    for index, row in input_df.iterrows():\n",
    "        response = fixed_graph.invoke({\n",
    "            \"question\": row[\"question\"]\n",
    "        })\n",
    "        answer.append(response[\"answer\"])\n",
    "\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f02d740-8189-46cd-ba85-f4e27b99a62c",
   "metadata": {},
   "source": [
    "Let's define some environment variables for MLflow to invoke AWS models. **The model evaluation will fail without these AWS credentials.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8c5dfb-9861-45d3-bed4-b0644cd5e444",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import os\n",
    "\n",
    "# Set AWS environment variables for MLflow\n",
    "os.environ[\"AWS_REGION\"] = boto3.session.Session().region_name\n",
    "os.environ[\"AWS_ACCESS_KEY_ID\"] = \"\" # Insert AWS access key here\n",
    "os.environ[\"AWS_SECRET_ACCESS_KEY\"] = \"\" # Insert AWS secret access key here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29cf2ed3-9306-48c8-85df-f9008d80e400",
   "metadata": {},
   "source": [
    "We are going to connect a managed MLflow tracking server to store results from the experiment.\n",
    "\n",
    "**Make sure that you already have an ML tracking server running in your SageMaker domain**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4286b05-31df-45ea-96ce-588971d18cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tracking_servers = [s['TrackingServerArn'] for s \n",
    "                    in boto3.client(\"sagemaker\").list_mlflow_tracking_servers()['TrackingServerSummaries']\n",
    "                    if s['IsActive'] == 'Active']\n",
    "\n",
    "if len(tracking_servers) < 1:\n",
    "    print(\"You don't have any active MLflow servers. Trying to find a server in the status 'Creating'...\")\n",
    "\n",
    "    r = boto3.client(\"sagemaker\").list_mlflow_tracking_servers(\n",
    "        TrackingServerStatus='Creating',\n",
    "    )['TrackingServerSummaries']\n",
    "\n",
    "    if len(r) < 1:\n",
    "        print(\"You don't have any MLflow server in the status 'Creating'. Run the next code cell to create a new one.\")\n",
    "        mlflow_arn = None\n",
    "        mlflow_name = None\n",
    "    else:\n",
    "        mlflow_arn = r[0]['TrackingServerArn']\n",
    "        mlflow_name = r[0]['TrackingServerName']\n",
    "        print(f\"You have an MLflow server {mlflow_arn} in the status 'Creating', going to use this one\")\n",
    "else:\n",
    "    mlflow_arn = tracking_servers[0]\n",
    "    mlflow_name = tracking_servers[0].split('/')[1]\n",
    "    print(f\"You have {len(tracking_servers)} running MLflow server(s). Get the first server ARN:{mlflow_arn}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ce9ca7-652c-4023-a2d7-531ed6fc85b2",
   "metadata": {},
   "source": [
    "Now we are going to run the evaluation using `mlflow.evaluate` and store the results in our tracking server. In this case, we are going to track both the latency in the evaluations and the correctness of the answer as evaluated using Claude 3 Sonnet (LLM as a judge). We are also going to add metrics from the chunking and embedding stages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e745ed11-f391-40cd-9435-64a81bad4f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from time import gmtime, strftime, sleep\n",
    "\n",
    "mlflow.set_tracking_uri(mlflow_arn)\n",
    "experiment_suffix = strftime('%d-%H-%M-%S', gmtime())\n",
    "experiment_name = f\"recursive-chunking-exp-{experiment_suffix}\"\n",
    "experiment = mlflow.set_experiment(experiment_name=experiment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2c73b8-105e-4426-ae16-508ce5658c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run evaluation\n",
    "with mlflow.start_run():\n",
    "    results = mlflow.evaluate(\n",
    "        model=recursive_model,\n",
    "        data=eval_df,\n",
    "        targets=\"answer\",\n",
    "        extra_metrics=[\n",
    "            mlflow.metrics.genai.answer_correctness(\n",
    "                model=\"bedrock:/anthropic.claude-3-sonnet-20240229-v1:0\",\n",
    "                parameters={\n",
    "                    \"temperature\": 0.2,\n",
    "                    \"max_tokens\": 256,\n",
    "                    \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "                },\n",
    "            ),\n",
    "            mlflow.metrics.latency(),\n",
    "        ],\n",
    "        evaluator_config={\n",
    "          \"col_mapping\": {\n",
    "              \"inputs\": \"question\",\n",
    "          }\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Store metrics from the evaluation\n",
    "    mlflow.log_metrics(results.metrics)\n",
    "    # Store other parameters\n",
    "    mlflow.log_param(\"Chunking strategy\", \"recursive\")\n",
    "    mlflow.log_param(\"Chunk size\", CHUNK_SIZE)\n",
    "    mlflow.log_param(\"Chunk overlap\", CHUNK_OVERLAP)\n",
    "    mlflow.log_param(\"Number of chunks\", NUM_CHUNKS_RECURSIVE)\n",
    "    mlflow.log_param(\"Embedding model ID\", EMBEDDING_MODEL_ID)\n",
    "    mlflow.log_param(\"Embedding model version\", EMBEDDING_MODEL_VERSION)\n",
    "    mlflow.log_param(\"Text generation model ID\", TEXT_GENERATION_MODEL_ID)\n",
    "    mlflow.log_param(\"Text generation model version\", TEXT_GENERATION_MODEL_VERSION)\n",
    "    mlflow.log_param(\"Chunking time seconds\", CHUNKING_TIME_RECURSIVE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6adb1d2-e680-4440-bb1b-b58bed487165",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from time import gmtime, strftime, sleep\n",
    "\n",
    "mlflow.set_tracking_uri(mlflow_arn)\n",
    "experiment_suffix = strftime('%d-%H-%M-%S', gmtime())\n",
    "experiment_name = f\"fixed-chunking-exp-{experiment_suffix}\"\n",
    "experiment = mlflow.set_experiment(experiment_name=experiment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13074f5-fbd4-400a-bf89-2fcabb7f93aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run evaluation\n",
    "with mlflow.start_run():\n",
    "    results = mlflow.evaluate(\n",
    "        model=fixed_model,\n",
    "        data=eval_df,\n",
    "        targets=\"answer\",\n",
    "        extra_metrics=[\n",
    "            mlflow.metrics.genai.answer_correctness(\n",
    "                model=\"bedrock:/anthropic.claude-3-sonnet-20240229-v1:0\",\n",
    "                parameters={\n",
    "                    \"temperature\": 0.2,\n",
    "                    \"max_tokens\": 256,\n",
    "                    \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "                },\n",
    "            ),\n",
    "            mlflow.metrics.latency(),\n",
    "        ],\n",
    "        evaluator_config={\n",
    "          \"col_mapping\": {\n",
    "              \"inputs\": \"question\",\n",
    "          }\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Store metrics from the evaluation\n",
    "    mlflow.log_metrics(results.metrics)\n",
    "    # Store other parameters\n",
    "    mlflow.log_param(\"Chunking strategy\", \"fixed\")\n",
    "    mlflow.log_param(\"Chunk size\", CHUNK_SIZE)\n",
    "    mlflow.log_param(\"Chunk overlap\", CHUNK_OVERLAP)\n",
    "    mlflow.log_param(\"Number of chunks\", NUM_CHUNKS_FIXED)\n",
    "    mlflow.log_param(\"Embedding model ID\", EMBEDDING_MODEL_ID)\n",
    "    mlflow.log_param(\"Embedding model version\", EMBEDDING_MODEL_VERSION)\n",
    "    mlflow.log_param(\"Text generation model ID\", TEXT_GENERATION_MODEL_ID)\n",
    "    mlflow.log_param(\"Text generation model version\", TEXT_GENERATION_MODEL_VERSION)\n",
    "    mlflow.log_param(\"Chunking time seconds\", CHUNKING_TIME_FIXED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8410e38-e6b8-462a-b10c-e5b0ab737a54",
   "metadata": {},
   "source": [
    "Results from the experiments have been loaded into the MLflow tracking server. Machine learning engineers can use MLflow experiments to compare different RAG configurations and see the effect of changing:\n",
    "\n",
    "- Prompt\n",
    "- Embedding model\n",
    "- Chunk size\n",
    "- Chunk overlap\n",
    "- Chunking strategy\n",
    "- Distance metric\n",
    "\n",
    "and others, and use this information to pick highly performant RAG solutions. Metrics can be viewed and compared in the MLflow tracking server:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e58b065b-4cb3-40f2-95ef-985867eae2cd",
   "metadata": {},
   "source": [
    "![MLflow Experiment Run](mlflow-experiment-run.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7de3ca5-7ef5-41de-969f-decc7ca0832f",
   "metadata": {},
   "source": [
    "## 12. Clean up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3234eed4-a8ca-4adb-a5df-f5873cbdb895",
   "metadata": {},
   "source": [
    "Let's delete the models, S3 buckets, and S3 vectors buckets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c93317ba-0c8c-4f15-ada5-c1f42260199a",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model_predictor.delete_endpoint()\n",
    "text_generation_predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e91d036-6111-4396-8d91-91d8b45c8041",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete S3 vectors bucket and indexes\n",
    "import boto3\n",
    "\n",
    "s3vectors = boto3.client(\"s3vectors\")\n",
    "\n",
    "s3vectors.delete_index(\n",
    "    vectorBucketName=VECTORS_BUCKET_NAME,\n",
    "    indexName=FIXED_CHUNKING_INDEX_NAME\n",
    ")\n",
    "s3vectors.delete_index(\n",
    "    vectorBucketName=VECTORS_BUCKET_NAME,\n",
    "    indexName=RECURSIVE_CHUNKING_INDEX_NAME\n",
    ")\n",
    "s3vectors.delete_vector_bucket(\n",
    "    vectorBucketName=VECTORS_BUCKET_NAME\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed86c8a-a857-4275-a26b-87c35a6d6661",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d845ab-5eb7-46e3-8393-a4cb97375304",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
